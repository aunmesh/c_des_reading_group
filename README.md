# C Design Lab Machine Learning Reading Group
* **Datetime** : 8:00 PM on Mon and Thr every week
* **Participants** : Asim, Hyung-gun, Seung-gun
* **WebEx link** : [https://purdue-student.webex.com/meet/chi45](https://purdue-student.webex.com/meet/chi45)

## Before 4 May:

**Hyung Gun**
* [3D Hand Pose Estimation in the Wild via Graph Refinement under Adversarial Learning](https://arxiv.org/pdf/1912.01875.pdf)

* [PoseFix: Model-agnostic General Human Pose Refinement Network](https://arxiv.org/abs/1812.03595)

* [Fast Online Object Tracking and Segmentation: A Unifying Approach](http://www.robots.ox.ac.uk/~qwang/SiamMask/ )

* [Learning feed-forward one-shot learners](https://arxiv.org/pdf/1606.05233.pdf )

* [Learning Pose Specific Representations by Predicting Different Views](https://arxiv.org/pdf/1804.03390v2.pdf )

* [Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild ](https://arxiv.org/pdf/2004.01946.pdf)

* [ViewAL: Active Learning With Viewpoint Entropy for Semantic Segmentation ](https://arxiv.org/pdf/1911.11789.pdf )

* [Temporal Cycle-Consistency Learning](https://arxiv.org/pdf/1904.07846.pdf )

* [PointRend: Image Segmentation as Rendering](https://arxiv.org/pdf/1912.08193.pdf )

* [Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision](https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/509/paper_camera_ready.pdf)


**Asim**
* [Learning by Association -- A Versatile Semi-Supervised Training Method for Neural Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Haeusser_Learning_by_Association_CVPR_2017_paper.pdf)

* [Active 6D Multi-Object Pose Estimation in Cluttered Scenarios with Deep Reinforcement Learning](https://arxiv.org/pdf/1910.08811.pdf)

* [Iterative Instance Segmentation](https://arxiv.org/pdf/1511.08498.pdf)

* [Learning to Generate Synthetic Data via Compositing](http://openaccess.thecvf.com/content_CVPR_2019/papers/Tripathi_Learning_to_Generate_Synthetic_Data_via_Compositing_CVPR_2019_paper.pdf)

* [Learning Loss for Active Learning](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yoo_Learning_Loss_for_Active_Learning_CVPR_2019_paper.pdf)

* [Improving the Robustness of Deep Neural Networks via Stability Training](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zheng_Improving_the_Robustness_CVPR_2016_paper.pdf)

* [How transferable are features in Deep Neural Networks](https://arxiv.org/pdf/1411.1792.pdf)

* [MMSS: Multi-Modal Sharable and Specific Feature Learning for RGB-D Object Recognition](http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_MMSS_Multi-Modal_Sharable_ICCV_2015_paper.pdf)

* [Learning Descriptors for Object Recognition and 3D Pose Estimation](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wohlhart_Learning_Descriptors_for_2015_CVPR_paper.pdf)



## 4 May

**Hyung Gun**
[Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction](https://arxiv.org/pdf/2004.13449.pdf)

> Uses photometric consistency to propagate annotations to un-annotated frames.

**Asim**
[Sim2real transfer learning for 3D human pose
estimation: motion to the rescue](https://papers.nips.cc/paper/9454-sim2real-transfer-learning-for-3d-human-pose-estimation-motion-to-the-rescue.pdf)

> Proposes using optical flow and 2d keypoints as input for pose estimation networks. Since these inputs are not affected by photorealism , hence can be trained well using synthetic data, without suffering much performance drop for real data.



## 6 May

**Hyung Gun**
[Self-Supervised GANs via Auxiliary Rotation Loss](https://arxiv.org/pdf/1811.11212.pdf)

> The role of self-supervision is to encourage the discriminator to learn meaningful feature repesentations which are not forgotten during training. The self-supervised GAN attains a similar performance to state-of-the-art conditional GAN.

**Asim**
[Cross-modal deep Variational hand pose estimation](http://openaccess.thecvf.com/content_cvpr_2018/papers/Spurr_Cross-Modal_Deep_Variational_CVPR_2018_paper.pdf)

> Uses different decoders from jointly obtained multi modal data to a joint latent space and does reconstruction from that. Is able to train using instances where all modalities are not available but a few are. Using this latent space they can train for hand pose estimation.

## 8 May

**Hyung Gun**
[HOnnotate: A method for 3D Annotation of Hand and Object Poses](https://arxiv.org/pdf/1907.01481.pdf)

**Asim**
[Self-Supervised Feature Learning by Learning to Spot Artifacts](http://openaccess.thecvf.com/content_cvpr_2018/papers/Jenni_Self-Supervised_Feature_Learning_CVPR_2018_paper.pdf)

## 12 May

**Hyung Gun**
[End-to-end Hand Mesh Recovery from a Monocular RGB Image](https://arxiv.org/pdf/1902.09305.pdf)

[Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge](https://arxiv.org/pdf/2004.00176.pdf)

**Asim**
[Synthesizing Training Images for Boosting Human 3D Pose Estimation](http://ai.stanford.edu/~haosu/papers/3DV_humanpose.pdf)



## 15 May
**Asim**
[Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation
Equivariant Layer](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Point-To-Pose_Voting_Based_Hand_Pose_Estimation_Using_Residual_Permutation_Equivariant_CVPR_2019_paper.pdf)


**Hyung Gun**
[Liquid Warping GAN: A Unified Framework for Human Motion Imitation,
Appearance Transfer and Novel View Synthesis](https://arxiv.org/pdf/1909.12224.pdf)

## 19 May

**Asim**
[Co-occurrent Features in Semantic Segmentation](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Co-Occurrent_Features_in_Semantic_Segmentation_CVPR_2019_paper.pdf)

**Hyung Gun**
[Imitation Learning for Human Pose Prediction](https://arxiv.org/pdf/1909.03449.pdf)

## 30 June

**Asim**
[What it thinks is important is important](https://arxiv.org/abs/1912.05699)

**Hyung Gun**
[Action Recognition from Single Timestamp Supervision in Untrimmed Videos](http://openaccess.thecvf.com/content_CVPR_2019/papers/Moltisanti_Action_Recognition_From_Single_Timestamp_Supervision_in_Untrimmed_Videos_CVPR_2019_paper.pdf)

**Seung Gun**
[Playing hard exploration games by watching YouTube](https://arxiv.org/pdf/1805.11592.pdf)
[fast weekly supervised action segmentation using mutual consistency](https://arxiv.org/pdf/1805.11592.pdf)

## 7 July

**Asim**
[Video Modeling with Correlation Networks](https://arxiv.org/abs/1906.03349)

[An Unsupervised Framework for Action Recognition Using Actemes](http://users.umiacs.umd.edu/~kale/accv2010.pdf)

**Hyung Gun**
[Deep progressive reinforcement learning for skeleton based action recognition](https://openaccess.thecvf.com/content_cvpr_2018/papers/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.pdf)

[Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation](https://openaccess.thecvf.com/content_CVPR_2020/papers/Arar_Unsupervised_Multi-Modal_Image_Registration_via_Geometry_Preserving_Image-to-Image_Translation_CVPR_2020_paper.pdf)

**Seung Gun**
[Playing hard exploration games by watching YouTube](https://arxiv.org/pdf/1805.11592.pdf)

<details markdown="1">
<summary>plot of Playing hard exploration games by watching YouTube</summary>

- Paper with youtube data
![Youtube](https://user-images.githubusercontent.com/37060326/86855774-036a3000-c0f6-11ea-9675-54dc281db287.png)

- Applying their method to Georgia Tech Egocentric Activity Datasets
![Cheese](https://user-images.githubusercontent.com/37060326/86856271-f00b9480-c0f6-11ea-81ad-00af01ae9946.png)
![Georgia](https://user-images.githubusercontent.com/37060326/86855766-fe0ce580-c0f5-11ea-8f97-6a97e6cd3261.png)


</details>


## 14 July

**Asim**

[Action2Vec: A Cross Modal Embedding Approach to Action Learning](https://arxiv.org/pdf/1901.00484.pdf)

**Hyung Gun**
[METAL_Minimum_Effort_Temporal_Activity_Localization_in Untrimmed Videos](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_METAL_Minimum_Effort_Temporal_Activity_Localization_in_Untrimmed_Videos_CVPR_2020_paper.pdf)

Description: Activity Localization using multi scale features and building a similarity pyramid.

**Seung Gun**
[METAL_Minimum_Effort_Temporal_Activity_Localization_in_Untrimmed_Videos](https://arxiv.org/pdf/1703.08132.pdf)
